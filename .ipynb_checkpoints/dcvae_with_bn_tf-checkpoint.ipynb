{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from util import random_mini_batches\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "\n",
    "st = tf.contrib.bayesflow.stochastic_tensor\n",
    "Normal = tf.contrib.distributions.Normal\n",
    "Bernoulli = tf.contrib.distributions.Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some constants\n",
    "LEARNING_RATE = 0.002\n",
    "BETA1 = 0.6\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "SAVE_SAMPLE_PERIOD = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x, alpha =0.2):\n",
    "    return tf.maximum(alpha*x,x)\n",
    "\n",
    "if not os.path.exists('samples'):\n",
    "    os.mkdir('samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        mi, mo,\n",
    "        apply_batch_norm,\n",
    "        filter_sz=5,stride=2,\n",
    "        f=tf.nn.relu,\n",
    "    ):\n",
    "        \n",
    "        self.W = tf.get_variable(\n",
    "            'W_%s' %name,\n",
    "            shape = (filter_sz, filter_sz, mi, mo),\n",
    "            initializer=tf.glorot_uniform_initializer(),\n",
    "        )\n",
    "        \n",
    "        self.b = tf.get_variable(\n",
    "            'b_%s' %name,\n",
    "            shape = (mo, ),\n",
    "            initializer=tf.zeros_initializer(),\n",
    "        )\n",
    "        \n",
    "        self.name = name\n",
    "        self.f = f\n",
    "        self.stride=stride\n",
    "        self.apply_batch_norm = apply_batch_norm\n",
    "    \n",
    "    def forward(self, X, reuse, is_training):\n",
    "        \n",
    "        conv_out = tf.nn.conv2d(\n",
    "            X,\n",
    "            self.W,\n",
    "            strides=[1,self.stride,self.stride,1],\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        conv_out = tf.nn.bias_add(conv_out, self.b)\n",
    "        \n",
    "        if self.apply_batch_norm:\n",
    "            \n",
    "            conv_out = tf.contrib.layers.batch_norm(\n",
    "                conv_out,\n",
    "                decay=0.9,\n",
    "                updates_collections=None,\n",
    "                epsilon=1e-5,\n",
    "                scale=True,\n",
    "                is_training = is_training,\n",
    "                reuse=reuse,\n",
    "                scope = self.name,\n",
    "            )\n",
    "            \n",
    "        return self.f(conv_out)\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        \n",
    "        self.session = session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvLayer(object):\n",
    "    \n",
    "    #fractionally strided convolution\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        mi, mo,\n",
    "        output_shape,\n",
    "        apply_batch_norm,\n",
    "        filter_sz=5, stride=2,\n",
    "        f=tf.nn.relu\n",
    "    ):\n",
    "        \n",
    "        self.W = tf.get_variable(\n",
    "            'W_%s' %name,\n",
    "            shape=(filter_sz, filter_sz, mo, mi),\n",
    "            initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        )\n",
    "        self.b = tf.get_variable(\n",
    "            'b_%s' %name,\n",
    "            shape=(mo,),\n",
    "            initializer=tf.zeros_initializer(),\n",
    "        )\n",
    "        \n",
    "        self.name = name\n",
    "        self.f = f\n",
    "        self.stride=stride\n",
    "        self.output_shape = output_shape\n",
    "        self.apply_batch_norm = apply_batch_norm\n",
    "        self.params = [self.W,self.b]\n",
    "\n",
    "    def forward(self, X, reuse, is_training):\n",
    "\n",
    "        conv_out = tf.nn.conv2d_transpose(\n",
    "            value=X,\n",
    "            filter=self.W,\n",
    "            output_shape=self.output_shape,\n",
    "            strides=[1, self.stride, self.stride, 1]\n",
    "        )\n",
    "\n",
    "        conv_out = tf.nn.bias_add(conv_out,self.b)\n",
    "        \n",
    "        if self.apply_batch_norm:\n",
    "            conv_out=tf.contrib.layers.batch_norm(\n",
    "                conv_out,\n",
    "                decay=0.9,\n",
    "                updates_collections=None,\n",
    "                epsilon=1e-5,\n",
    "                scale=True,\n",
    "                is_training = is_training,\n",
    "                reuse=reuse,\n",
    "                scope = self.name,\n",
    "            )\n",
    "        return self.f(conv_out)\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        \n",
    "        self.session = session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 mi, mo,\n",
    "                 apply_batch_norm,\n",
    "                 f=tf.nn.relu):\n",
    "        \n",
    "        #self.mi = mi\n",
    "        #self.mo = mo\n",
    "        \n",
    "        self.W = tf.get_variable(\n",
    "            'W_%s' %name,\n",
    "            shape=(mi, mo),\n",
    "            initializer=tf.glorot_normal_initializer(),\n",
    "        )\n",
    "        self.b = tf.get_variable(\n",
    "            'b_%s' %name,\n",
    "            shape=(mo, ),\n",
    "            initializer=tf.zeros_initializer(),\n",
    "        )\n",
    "        \n",
    "        self.f = f\n",
    "        self.name = name\n",
    "        self.apply_batch_norm = apply_batch_norm\n",
    "        \n",
    "        \n",
    "    def forward(self, X, reuse, is_training):\n",
    "        \n",
    "        Z = tf.matmul(X,self.W) + self.b\n",
    "        \n",
    "        if self.apply_batch_norm:\n",
    "            \n",
    "            Z = tf.contrib.layers.batch_norm(\n",
    "                Z,\n",
    "                decay=0.9,\n",
    "                updates_collections=None,\n",
    "                epsilon=1e-5,\n",
    "                scale=True,\n",
    "                is_training = is_training,\n",
    "                reuse=reuse,\n",
    "                scope = self.name,\n",
    "            )\n",
    "            \n",
    "        return self.f(Z)\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        \n",
    "        self.session = session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder:\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_W, n_C, e_sizes, d_sizes):\n",
    "        \n",
    "        #size of every layer in the encoder\n",
    "        #up to the latent layer, decoder\n",
    "        #will have reverse shape\n",
    "        self.n_W = n_W\n",
    "        self.n_C = n_C\n",
    "        \n",
    "        self.e_sizes = e_sizes\n",
    "        self.d_sizes = d_sizes\n",
    "        self.latent_dims = e_sizes['z']\n",
    "\n",
    "        \n",
    "        self.X = tf.placeholder(\n",
    "            tf.float32,\n",
    "            shape=(None, n_W, n_W, n_C),\n",
    "            name='X'\n",
    "        )\n",
    "        \n",
    "        self.batch_sz = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=(),\n",
    "            name='batch_sz'\n",
    "        )\n",
    "        \n",
    "        #builds the encoder and outputs a Z distribution\n",
    "        self.Z = self.build_encoder(self.X, self.e_sizes)\n",
    "        \n",
    "        #builds decoder from Z distribution\n",
    "        logits = self.build_decoder(self.Z, self.d_sizes)\n",
    "        \n",
    "        #builds X_hat distribution from decoder output\n",
    "        self.X_hat_distribution = Bernoulli(logits=logits)\n",
    "        \n",
    "        \n",
    "        #posterior predictive\n",
    "        \n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            scope.reuse_variables\n",
    "            self.Z_dist = self.encode(\n",
    "                self.X, reuse=True, is_training=False,\n",
    "            )#self.X or something on purpose?\n",
    "\n",
    "                                                   \n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "            scope.reuse_variables()\n",
    "            sample_logits = self.decode(\n",
    "                self.Z_dist, reuse=True, is_training=False,\n",
    "            )\n",
    "            \n",
    "        self.posterior_predictive_dist = Bernoulli(logits=sample_logits)\n",
    "        self.posterior_predictive = self.posterior_predictive_dist.sample()\n",
    "        self.posterior_predictive_probs = tf.nn.sigmoid(sample_logits)\n",
    "        \n",
    "        #prior predictive from prob\n",
    "\n",
    "        standard_normal = Normal(\n",
    "          loc=np.zeros(self.latent_dims, dtype=np.float32),\n",
    "          scale=np.ones(self.latent_dims, dtype=np.float32)\n",
    "        )\n",
    "\n",
    "        Z_std = standard_normal.sample(1)\n",
    "\n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "            scope.reuse_variables()\n",
    "            logits_from_prob = self.decode(\n",
    "                Z_std, reuse=True, is_training=False,\n",
    "            )\n",
    "        \n",
    "        prior_predictive_dist = Bernoulli(logits=logits_from_prob)\n",
    "        self.prior_predictive = prior_predictive_dist.sample()\n",
    "        self.prior_predictive_probs = tf.nn.sigmoid(logits_from_prob)\n",
    "\n",
    "\n",
    "        # prior predictive from input\n",
    "\n",
    "        self.Z_input = tf.placeholder(tf.float32, shape=(None, self.latent_dims))\n",
    "        \n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "            scope.reuse_variables()    \n",
    "            logits_from_input = self.decode(\n",
    "                self.Z_input, reuse=True, is_training=False,\n",
    "            )\n",
    "        \n",
    "        input_predictive_dist = Bernoulli(logits=logits_from_input)\n",
    "        self.prior_predictive_from_input= input_predictive_dist.sample()\n",
    "        self.prior_predictive_from_input_probs = tf.nn.sigmoid(logits_from_input)\n",
    "\n",
    "        \n",
    "        #cost\n",
    "        kl = tf.reduce_sum(\n",
    "            tf.contrib.distributions.kl_divergence(\n",
    "                self.Z.distribution,\n",
    "                standard_normal),\n",
    "            1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        expected_log_likelihood = tf.reduce_sum(\n",
    "              self.X_hat_distribution.log_prob(self.X),\n",
    "              1\n",
    "        )\n",
    "        \n",
    "        self.elbo = tf.reduce_sum(expected_log_likelihood - kl)\n",
    "        self.train_op = tf.train.AdamOptimizer(\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            beta1=BETA1,\n",
    "        ).minimize(-self.elbo)\n",
    "\n",
    "    def build_encoder(self, X, e_sizes):\n",
    "        \n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            \n",
    "            M_in = self.n_C\n",
    "            dim = self.n_W\n",
    "            \n",
    "            self.e_conv_layers=[]\n",
    "            count = 0\n",
    "            \n",
    "            for M_out, filter_sz, stride, apply_batch_norm in e_sizes['conv_layers']:\n",
    "                \n",
    "                name = 'e_conv_layer_%s' %count\n",
    "                count += 1\n",
    "                \n",
    "                layer = ConvLayer(name, M_in, M_out, apply_batch_norm, filter_sz, stride, lrelu)\n",
    "                self.e_conv_layers.append(layer)\n",
    "                M_in = M_out\n",
    "                \n",
    "                #print('Dim:', dim)\n",
    "                dim = int(np.ceil(float(dim)/stride))\n",
    "            \n",
    "            M_in = M_in*dim*dim\n",
    "            \n",
    "            self.e_dense_layers=[]\n",
    "            \n",
    "            for M_out, apply_batch_norm in e_sizes['dense_layers']:\n",
    "                \n",
    "                name = 'e_dense_layer_%s' %count\n",
    "                count +=1\n",
    "                \n",
    "                layer = DenseLayer(name, M_in, M_out, apply_batch_norm, lrelu)\n",
    "                self.e_dense_layers.append(layer)\n",
    "                \n",
    "                M_in = M_out\n",
    "        \n",
    "            #no activation of last layer and need 2\n",
    "            #times as many units (M means and M stddevs)\n",
    "            name = 'e_conv_layer_%s' %count\n",
    "            last_enc_layer = DenseLayer(name, M_in, 2*self.latent_dims, apply_batch_norm=False, f=lambda x: x)\n",
    "            self.e_dense_layers.append(last_enc_layer)\n",
    "            \n",
    "            return self.encode(X)\n",
    "        \n",
    "    def encode(self, X, reuse=None, is_training=True):\n",
    "        #propagate X until end of encoder\n",
    "        output=X\n",
    "\n",
    "        for layer in self.e_conv_layers:\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "        \n",
    "        output = tf.contrib.layers.flatten(output)\n",
    "        \n",
    "        for layer in self.e_dense_layers:\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "        \n",
    "        \n",
    "        #get means and stddev from last encoder layer\n",
    "        self.means = output[:, :self.latent_dims]\n",
    "        self.stddev = tf.nn.softplus(output[:,self.latent_dims:])+1e-6\n",
    "        \n",
    "        # get a sample of Z, we need to use a stochastic tensor\n",
    "        # in order for the errors to be backpropagated past this point\n",
    "        \n",
    "        with st.value_type(st.SampleValue()):\n",
    "            Z = st.StochasticTensor(Normal(loc=self.means, scale=self.stddev))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "        #build decoder\n",
    "    def build_decoder(self, Z, d_sizes):\n",
    "        \n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "            \n",
    "            dims=[self.n_W]\n",
    "            dim = self.n_W\n",
    "            \n",
    "            for _, _, stride, _ in reversed(d_sizes['conv_layers']):\n",
    "                dim = int(np.ceil(float(dim)/stride))\n",
    "                dims.append(dim)\n",
    "            \n",
    "            dims = list(reversed(dims))\n",
    "            #print('Decoder dims:', dims)\n",
    "            self.d_dims = dims\n",
    "            \n",
    "            M_in = self.latent_dims\n",
    "            self.d_dense_layers =[]\n",
    "            \n",
    "\n",
    "            count=0\n",
    "            for M_out, apply_batch_norm in d_sizes['dense_layers']:\n",
    "                \n",
    "                name = 'd_dense_layer_%s' %count\n",
    "                count +=1\n",
    "                \n",
    "                layer = DenseLayer(name, M_in, M_out, apply_batch_norm)\n",
    "                self.d_dense_layers.append(layer)\n",
    "                M_in = M_out\n",
    "                \n",
    "            M_out = d_sizes['projection']*dims[0]*dims[0]\n",
    "\n",
    "\n",
    "            #final dense layer\n",
    "            name = 'dec_layer_%s' %count\n",
    "            last_dec_layer = DenseLayer(name, M_in, M_out, not d_sizes['bn_after_project'])\n",
    "            self.d_dense_layers.append(last_dec_layer)\n",
    "            \n",
    "            \n",
    "            #fractionally strided layers\n",
    "            \n",
    "            M_in = d_sizes['projection']\n",
    "            self.d_conv_layers=[]\n",
    "            \n",
    "            #unactivated output\n",
    "            num_relus = len(d_sizes['conv_layers'])-1\n",
    "            #activation_functions = [tf.nn.relu]*num_relus +[d_sizes['output_activation']]\n",
    "            activation_functions = [tf.nn.relu]*num_relus +[lambda x: x]           \n",
    "\n",
    "\n",
    "            for i in range(len(d_sizes['conv_layers'])):\n",
    "               \n",
    "                name = 'd_conv_layer_%s' %i\n",
    "                M_out, filter_sz, stride, apply_batch_norm = d_sizes['conv_layers'][i]\n",
    "                f = activation_functions[i]\n",
    "                \n",
    "                output_shape = [self.batch_sz, dims[i+1], dims[i+1], M_out]\n",
    "                #print(\"M_in:\", M_in, \"M_out:\", M_out, \"output_shape:\", output_shape)\n",
    "                \n",
    "                layer = DeconvLayer(\n",
    "                    name, M_in, M_out, output_shape, apply_batch_norm, filter_sz, stride, f\n",
    "                )\n",
    "                self.d_conv_layers.append(layer)\n",
    "                \n",
    "            self.d_sizes = d_sizes\n",
    "            \n",
    "            return self.decode(Z)\n",
    "    \n",
    "    def decode(self, Z, reuse=None, is_training=True):\n",
    "        \n",
    "        #dense layers\n",
    "        output = Z\n",
    "        \n",
    "        for layer in self.d_dense_layers:\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "\n",
    "        output = tf.reshape(\n",
    "            output,\n",
    "            [-1, self.d_dims[0],self.d_dims[0],self.d_sizes['projection']]\n",
    "        )\n",
    "\n",
    "        if self.d_sizes['bn_after_project']:\n",
    "            output = tf.contrib.layers.batch_norm(\n",
    "            output,\n",
    "            decay=0.9, \n",
    "            updates_collections=None,\n",
    "            epsilon=1e-5,\n",
    "            scale=True,\n",
    "            is_training=is_training,\n",
    "            reuse=reuse,\n",
    "            scope='bn_after_project'\n",
    "        )        \n",
    "        #passing to fs-convolutional layers   \n",
    "        \n",
    "        for layer in self.d_conv_layers:\n",
    "\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        \n",
    "        self.session = session\n",
    "        \n",
    "        for layer in self.e_conv_layers:\n",
    "            layer.set_session(session)\n",
    "        for layer in self.e_dense_layers:\n",
    "            layer.set_session(session)\n",
    "            \n",
    "        for layer in self.d_dense_layers:\n",
    "            layer.set_session(session) \n",
    "        for layer in self.d_conv_layers:\n",
    "            layer.set_session(session)  \n",
    "        \n",
    "    def fit(self, X):\n",
    "        costs = []\n",
    "        \n",
    "        n_batches = len(X)//BATCH_SIZE\n",
    "        print(\"# batches\", n_batches)\n",
    "        \n",
    "        total_iters=0\n",
    "        \n",
    "        for i in range(EPOCHS):\n",
    "            print(\"Epoch\", i)\n",
    "            np.random.shuffle(X)\n",
    "            \n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                X_batch = X[j*BATCH_SIZE:(j+1)*BATCH_SIZE]\n",
    "                _, c = self.session.run((self.train_op,self.elbo),feed_dict={self.X:X_batch, self.batch_sz:BATCH_SIZE})\n",
    "                c /= BATCH_SIZE\n",
    "                costs.append(c)\n",
    "                \n",
    "                if j % 250 == 0:\n",
    "                    print(\"on iter %d, cost: %.3f\" %(j, c))\n",
    "                \n",
    "                total_iters +=1\n",
    "                if total_iters % SAVE_SAMPLE_PERIOD == 0:\n",
    "                    print('Saving a sample...')\n",
    "                    samples = self.sample(64)\n",
    "                    \n",
    "                    d = self.n_W\n",
    "                    flat_image = np.empty((8*d,8*d))\n",
    "                    \n",
    "                    k=0\n",
    "                    for i in range(8):\n",
    "                        for j in range(8):\n",
    "                            flat_image[i*d:(i+1)*d, j*d:(j+1)*d] = samples[k].reshape(d, d)\n",
    "                            k+=1\n",
    "                            \n",
    "                    plt.imshow(flat_image, cmap='gray')\n",
    "                    \n",
    "                    sp.misc.imsave(\n",
    "                        'samples/samples_at_iter_%d.png' % total_iters,\n",
    "                        flat_image,\n",
    "                    )\n",
    "            plt.clf()\n",
    "            plt.plot(costs, label='cost vs iteration')\n",
    "            plt.legend()\n",
    "            plt.savefig('cost vs iteration.png')\n",
    "            \n",
    "    def sample(self, n):\n",
    "        Z = np.random.uniform(-1,1, size=(n,self.latent_dims))\n",
    "        samples = self.session.run(\n",
    "          self.prior_predictive_from_input_probs,\n",
    "          feed_dict={self.Z_input: Z, self.batch_sz: n}\n",
    "        )\n",
    "        return samples\n",
    "\n",
    "    def prior_predictive_with_input(self, Z):\n",
    "        return self.session.run(\n",
    "          self.prior_predictive_from_input_probs,\n",
    "          feed_dict={self.Z_input: Z}\n",
    "        )\n",
    "\n",
    "    def posterior_predictive_sample(self, X):\n",
    "        # returns a sample from p(x_new | X)\n",
    "        return self.session.run(self.posterior_predictive_probs, feed_dict={self.X: X})\n",
    "\n",
    "    def prior_predictive_sample_with_probs(self):\n",
    "        # returns a sample from p(x_new | z), z ~ N(0, 1)\n",
    "        return self.session.run((self.prior_predictive, self.prior_predictive_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vae():\n",
    "\n",
    "    X_train = mnist.train.images\n",
    "    #X_train = X_train/255\n",
    "    \n",
    "    X_train = X_train.reshape(len(X_train),28,28,1)\n",
    "    X_train = (X_train>0.5).astype(np.float32)\n",
    "    \n",
    "    X_test = mnist.test.images\n",
    "    #X_test = X_test/255\n",
    "\n",
    "    X_test = X_test.reshape(len(X_test),28,28,1)\n",
    "    X_test = (X_test>0.5).astype(np.float32)\n",
    "    \n",
    "    n_W = X_train.shape[1]\n",
    "    n_C = X_train.shape[-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    e_sizes = {\n",
    "        'conv_layers': [(2, 5, 2, False), (64, 5, 2, True)],\n",
    "        'dense_layers': [(1024, True)],'z': 100\n",
    "    }\n",
    "    \n",
    "    d_sizes = {\n",
    "        'projection': 128,\n",
    "        'bn_after_project': False,\n",
    "        'conv_layers': [(128, 5, 2, True), (n_C, 5, 2, False)],\n",
    "        'dense_layers': [(1024, True)],\n",
    "        'output_activation': tf.sigmoid,\n",
    "    }\n",
    "    tf.reset_default_graph()\n",
    "    vae = VariationalAutoencoder(n_W, n_C, e_sizes, d_sizes)\n",
    "    # set up session and variables for later\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        vae.set_session(sess)\n",
    "        vae.fit(X_train)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "        \n",
    "            i = np.random.choice(len(X_test))\n",
    "            x = X_test[i]\n",
    "            im = vae.posterior_predictive_sample([x]).reshape(28, 28)\n",
    "            \n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "            plt.title(\"Original\")\n",
    "            \n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(im, cmap='gray')\n",
    "            plt.title(\"Sampled\")\n",
    "            plt.show()\n",
    "\n",
    "            ans = input(\"Generate another?\")\n",
    "            if ans and ans[0] in ('n' or 'N'):\n",
    "                done = True\n",
    "\n",
    "  # plot output from random samples in latent space\n",
    "        done = False\n",
    "        while not done:\n",
    "        \n",
    "            im, probs = vae.prior_predictive_sample_with_probs()\n",
    "            \n",
    "            im = im.reshape(28, 28)\n",
    "            \n",
    "            probs = probs.reshape(28, 28)\n",
    "            \n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(im, cmap='gray')\n",
    "            plt.title(\"Prior predictive sample\")\n",
    "            \n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(probs, cmap='gray')\n",
    "            plt.title(\"Prior predictive probs\")\n",
    "            plt.show()\n",
    "\n",
    "            ans = input(\"Generate another?\")\n",
    "            if ans and ans[0] in ('n' or 'N'):\n",
    "                done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batches 859\n",
      "Epoch 0\n",
      "on iter 0, cost: -155253.891\n",
      "Saving a sample...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.misc' has no attribute 'imsave'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1c2b3e13dba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtest_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-f56086fc5d5a>\u001b[0m in \u001b[0;36mtest_vae\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ef7cb6b367bf>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                     sp.misc.imsave(\n\u001b[0m\u001b[1;32m    335\u001b[0m                         \u001b[0;34m'samples/samples_at_iter_%d.png'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtotal_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                         \u001b[0mflat_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.misc' has no attribute 'imsave'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    test_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
