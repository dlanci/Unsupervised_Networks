{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from util import random_mini_batches\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "\n",
    "st = tf.contrib.bayesflow.stochastic_tensor\n",
    "Normal = tf.contrib.distributions.Normal\n",
    "Bernoulli = tf.contrib.distributions.Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(object):\n",
    "    \n",
    "    def __init__(self, mi, mo, f=tf.nn.relu):\n",
    "        \n",
    "        #self.mi = mi\n",
    "        #self.mo = mo\n",
    "        \n",
    "        self.W = tf.Variable(tf.random_normal(shape=(mi,mo))*2/np.sqrt(mi))\n",
    "        self.b = tf.Variable(np.zeros(mo).astype(np.float32))\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        return self.f(tf.matmul(X,self.W) + self.b)\n",
    "    \n",
    "    def set_session(self, session):\n",
    "        \n",
    "        self.session = session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder:\n",
    "    \n",
    "    \n",
    "    def __init__(self, D, hidden_layer_sizes):\n",
    "        \n",
    "        #size of every layer in the encoder\n",
    "        #up to the latent layer, decoder\n",
    "        #will have reverse shape\n",
    "        \n",
    "        self.encoder_layers = []\n",
    "        self.decoder_layers = []\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, D))\n",
    "        \n",
    "        #build encoder\n",
    "        M_in = D\n",
    "        for M_out in hidden_layer_sizes[:-1]:\n",
    "            layer = DenseLayer(M_in, M_out)\n",
    "            self.encoder_layers.append(layer)\n",
    "            M_in = M_out\n",
    "        \n",
    "        #no activation of last layer and need 2\n",
    "        #times as many units (M means and M stddevs)\n",
    "        \n",
    "        M = hidden_layer_sizes[-1]\n",
    "        \n",
    "        last_enc_layer = DenseLayer(M_in, 2*M, f=lambda x: x)\n",
    "        self.encoder_layers.append(last_enc_layer)\n",
    "        \n",
    "        #propagate X until end of encoder\n",
    "        current_layer_value=self.X\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            current_layer_value = layer.forward(current_layer_value)\n",
    "        \n",
    "        #get means and stddev from last encoder layer\n",
    "        self.means = current_layer_value[:, :M]\n",
    "        self.stddev = tf.nn.softplus(current_layer_value[:,M:])+1e-6\n",
    "        \n",
    "        # get a sample of Z, we need to use a stochastic tensor\n",
    "        # in order for the errors to be backpropagated past this point\n",
    "        \n",
    "        with st.value_type(st.SampleValue()):\n",
    "            self.Z = st.StochasticTensor(Normal(loc=self.means, scale=self.stddev))\n",
    "            \n",
    "        #build decoder\n",
    "        \n",
    "        M_in = M\n",
    "        for M_out in reversed(hidden_layer_sizes[:-1]):\n",
    "            layer = DenseLayer(M_in, M_out)\n",
    "            self.decoder_layers.append(layer)\n",
    "            M_in = M_out\n",
    "\n",
    "        # the decoder's final layer should technically go through a sigmoid\n",
    "        # so that the final output is a binary probability (e.g. Bernoulli)\n",
    "        # but Bernoulli accepts logits (pre-sigmoid) so we will take those\n",
    "        # so no activation function is needed at the final layer\n",
    "        last_dec_layer = DenseLayer(M_in, D, f=lambda x: x)\n",
    "        self.decoder_layers.append(last_dec_layer)\n",
    "        \n",
    "        current_layer_value = self.Z\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            current_layer_value = layer.forward(current_layer_value)\n",
    "        \n",
    "        logits = current_layer_value\n",
    "        posterior_predictive_logits = logits\n",
    "        \n",
    "        # get the output\n",
    "        self.X_hat_distribution = Bernoulli(logits=logits)\n",
    "        \n",
    "        \n",
    "        #the posterior predictive sample\n",
    "        # take samples from X_hat\n",
    "        self.posterior_predictive = self.X_hat_distribution.sample()\n",
    "        self.posterior_predictive_probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "        # the prior predictive sample\n",
    "        # take sample from a Z ~ N(0, 1)\n",
    "        # and put it through the decoder\n",
    "\n",
    "        standard_normal = Normal(\n",
    "          loc=np.zeros(M, dtype=np.float32),\n",
    "          scale=np.ones(M, dtype=np.float32)\n",
    "        )\n",
    "\n",
    "        Z_std = standard_normal.sample(1)\n",
    "        current_layer_value=Z_std\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            current_layer_value = layer.forward(current_layer_value)\n",
    "            \n",
    "        logits = current_layer_value\n",
    "        \n",
    "        prior_predictive_dist = Bernoulli(logits=logits)\n",
    "        self.prior_predictive = prior_predictive_dist.sample()\n",
    "        self.prior_predictive_probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "\n",
    "        # prior predictive from input\n",
    "        # only used for generating visualization\n",
    "        self.Z_input = tf.placeholder(tf.float32, shape=(None, M))\n",
    "        current_layer_value = self.Z_input\n",
    "        for layer in self.decoder_layers:\n",
    "            current_layer_value = layer.forward(current_layer_value)\n",
    "            \n",
    "        logits = current_layer_value\n",
    "        self.prior_predictive_from_input_probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "\n",
    "        # now build the cost\n",
    "        kl = tf.reduce_sum(\n",
    "            tf.contrib.distributions.kl_divergence(\n",
    "                self.Z.distribution,\n",
    "                standard_normal),\n",
    "            1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        expected_log_likelihood = tf.reduce_sum(\n",
    "              self.X_hat_distribution.log_prob(self.X),\n",
    "              1\n",
    "        )\n",
    "        \n",
    "        self.elbo = tf.reduce_sum(expected_log_likelihood - kl)\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(-self.elbo)\n",
    "\n",
    "        \n",
    "    def fit(self, X, epochs = 30, batch_sz = 64):\n",
    "        costs = []\n",
    "        n_batches = len(X)//batch_sz\n",
    "        print(\"# batches\", n_batches)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            print(\"epoch\", i)\n",
    "            np.random.shuffle(X)\n",
    "            \n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                X_batch = X[j*batch_sz:(j+1)*batch_sz]\n",
    "                _, c = self.session.run((self.train_op,self.elbo),feed_dict={self.X:X_batch})\n",
    "                c /= batch_sz\n",
    "                costs.append(c)\n",
    "                \n",
    "                if j % 250 == 0:\n",
    "                    print(\"on iter %d, cost: %.3f\" %(j, c))\n",
    "                \n",
    "            plt.plot(costs)\n",
    "            plt.show\n",
    "            \n",
    "    def set_session(self, session):\n",
    "        \n",
    "        self.session = session\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            layer.set_session(session)\n",
    "            \n",
    "        for layer in self.decoder_layers:\n",
    "            layer.set_session(session)  \n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.session.run(\n",
    "          self.means,\n",
    "          feed_dict={self.X: X}\n",
    "        )\n",
    "\n",
    "    def prior_predictive_with_input(self, Z):\n",
    "        return self.session.run(\n",
    "          self.prior_predictive_from_input_probs,\n",
    "          feed_dict={self.Z_input: Z}\n",
    "        )\n",
    "\n",
    "    def posterior_predictive_sample(self, X):\n",
    "        # returns a sample from p(x_new | X)\n",
    "        return self.session.run(self.posterior_predictive, feed_dict={self.X: X})\n",
    "\n",
    "    def prior_predictive_sample_with_probs(self):\n",
    "        # returns a sample from p(x_new | z), z ~ N(0, 1)\n",
    "        return self.session.run((self.prior_predictive, self.prior_predictive_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vae():\n",
    "\n",
    "    X_train = mnist.train.images\n",
    "    #X_train = X_train/255\n",
    "    X_train = (X_train>0.5).astype(np.float32)\n",
    "    \n",
    "    X_test = mnist.test.images\n",
    "    #X_test = X_test/255\n",
    "    X_test = (X_test>0.5).astype(np.float32)\n",
    "\n",
    "    \n",
    "    N, D = X_train.shape\n",
    "    \n",
    "    vae = VariationalAutoencoder(D, [300,200,100])\n",
    "    # set up session and variables for later\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init_op)\n",
    "        vae.set_session(sess)\n",
    "        vae.fit(X_train)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "        \n",
    "            i = np.random.choice(len(X_test))\n",
    "            x = X_test[i]\n",
    "            im = vae.posterior_predictive_sample([x]).reshape(28, 28)\n",
    "            \n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "            plt.title(\"Original\")\n",
    "            \n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(im, cmap='gray')\n",
    "            plt.title(\"Sampled\")\n",
    "            plt.show()\n",
    "\n",
    "            ans = input(\"Generate another?\")\n",
    "            if ans and ans[0] in ('n' or 'N'):\n",
    "                done = True\n",
    "\n",
    "  # plot output from random samples in latent space\n",
    "        done = False\n",
    "        while not done:\n",
    "        \n",
    "            im, probs = vae.prior_predictive_sample_with_probs()\n",
    "            \n",
    "            im = im.reshape(28, 28)\n",
    "            \n",
    "            probs = probs.reshape(28, 28)\n",
    "            \n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(im, cmap='gray')\n",
    "            plt.title(\"Prior predictive sample\")\n",
    "            \n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(probs, cmap='gray')\n",
    "            plt.title(\"Prior predictive probs\")\n",
    "            plt.show()\n",
    "\n",
    "            ans = input(\"Generate another?\")\n",
    "            if ans and ans[0] in ('n' or 'N'):\n",
    "                done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batches 859\n",
      "epoch 0\n",
      "on iter 0, cost: -2996.807\n",
      "on iter 250, cost: -203.675\n",
      "on iter 500, cost: -174.288\n",
      "on iter 750, cost: -162.982\n",
      "epoch 1\n",
      "on iter 0, cost: -163.229\n",
      "on iter 250, cost: -153.923\n",
      "on iter 500, cost: -151.408\n",
      "on iter 750, cost: -141.222\n",
      "epoch 2\n",
      "on iter 0, cost: -133.047\n",
      "on iter 250, cost: -143.415\n",
      "on iter 500, cost: -124.103\n",
      "on iter 750, cost: -135.073\n",
      "epoch 3\n",
      "on iter 0, cost: -122.696\n",
      "on iter 250, cost: -117.397\n",
      "on iter 500, cost: -124.299\n",
      "on iter 750, cost: -119.466\n",
      "epoch 4\n",
      "on iter 0, cost: -117.893\n",
      "on iter 250, cost: -119.795\n",
      "on iter 500, cost: -108.713\n",
      "on iter 750, cost: -104.135\n",
      "epoch 5\n",
      "on iter 0, cost: -110.979\n",
      "on iter 250, cost: -106.492\n",
      "on iter 500, cost: -106.563\n",
      "on iter 750, cost: -116.225\n",
      "epoch 6\n",
      "on iter 0, cost: -106.370\n",
      "on iter 250, cost: -104.947\n",
      "on iter 500, cost: -102.465\n",
      "on iter 750, cost: -104.997\n",
      "epoch 7\n",
      "on iter 0, cost: -98.327\n",
      "on iter 250, cost: -99.797\n",
      "on iter 500, cost: -104.083\n",
      "on iter 750, cost: -101.275\n",
      "epoch 8\n",
      "on iter 0, cost: -97.253\n",
      "on iter 250, cost: -94.620\n",
      "on iter 500, cost: -99.501\n",
      "on iter 750, cost: -98.385\n",
      "epoch 9\n",
      "on iter 0, cost: -107.921\n",
      "on iter 250, cost: -98.593\n",
      "on iter 500, cost: -107.719\n",
      "on iter 750, cost: -98.946\n",
      "epoch 10\n",
      "on iter 0, cost: -101.454\n",
      "on iter 250, cost: -95.759\n",
      "on iter 500, cost: -97.252\n",
      "on iter 750, cost: -102.075\n",
      "epoch 11\n",
      "on iter 0, cost: -96.317\n",
      "on iter 250, cost: -92.770\n",
      "on iter 500, cost: -96.139\n",
      "on iter 750, cost: -96.109\n",
      "epoch 12\n",
      "on iter 0, cost: -96.718\n",
      "on iter 250, cost: -99.421\n",
      "on iter 500, cost: -93.709\n",
      "on iter 750, cost: -94.760\n",
      "epoch 13\n",
      "on iter 0, cost: -93.864\n",
      "on iter 250, cost: -97.772\n",
      "on iter 500, cost: -98.123\n",
      "on iter 750, cost: -90.342\n",
      "epoch 14\n",
      "on iter 0, cost: -94.292\n",
      "on iter 250, cost: -91.932\n",
      "on iter 500, cost: -93.358\n",
      "on iter 750, cost: -92.076\n",
      "epoch 15\n",
      "on iter 0, cost: -94.983\n",
      "on iter 250, cost: -86.502\n",
      "on iter 500, cost: -90.916\n",
      "on iter 750, cost: -85.689\n",
      "epoch 16\n",
      "on iter 0, cost: -91.924\n",
      "on iter 250, cost: -92.069\n",
      "on iter 500, cost: -94.839\n",
      "on iter 750, cost: -99.196\n",
      "epoch 17\n",
      "on iter 0, cost: -97.120\n",
      "on iter 250, cost: -86.795\n",
      "on iter 500, cost: -95.382\n",
      "on iter 750, cost: -92.895\n",
      "epoch 18\n",
      "on iter 0, cost: -94.101\n",
      "on iter 250, cost: -96.520\n",
      "on iter 500, cost: -91.935\n",
      "on iter 750, cost: -91.873\n",
      "epoch 19\n",
      "on iter 0, cost: -88.888\n",
      "on iter 250, cost: -95.857\n",
      "on iter 500, cost: -92.055\n",
      "on iter 750, cost: -98.455\n",
      "epoch 20\n",
      "on iter 0, cost: -85.540\n",
      "on iter 250, cost: -89.959\n",
      "on iter 500, cost: -91.374\n",
      "on iter 750, cost: -87.793\n",
      "epoch 21\n",
      "on iter 0, cost: -95.638\n",
      "on iter 250, cost: -91.145\n",
      "on iter 500, cost: -87.295\n",
      "on iter 750, cost: -95.744\n",
      "epoch 22\n",
      "on iter 0, cost: -91.256\n",
      "on iter 250, cost: -85.976\n",
      "on iter 500, cost: -92.370\n",
      "on iter 750, cost: -88.274\n",
      "epoch 23\n",
      "on iter 0, cost: -94.285\n",
      "on iter 250, cost: -85.302\n",
      "on iter 500, cost: -84.719\n",
      "on iter 750, cost: -88.915\n",
      "epoch 24\n",
      "on iter 0, cost: -83.932\n",
      "on iter 250, cost: -91.085\n",
      "on iter 500, cost: -87.488\n",
      "on iter 750, cost: -85.309\n",
      "epoch 25\n",
      "on iter 0, cost: -90.069\n",
      "on iter 250, cost: -88.719\n",
      "on iter 500, cost: -88.941\n",
      "on iter 750, cost: -88.089\n",
      "epoch 26\n",
      "on iter 0, cost: -84.563\n",
      "on iter 250, cost: -92.515\n",
      "on iter 500, cost: -88.184\n",
      "on iter 750, cost: -90.677\n",
      "epoch 27\n",
      "on iter 0, cost: -89.038\n",
      "on iter 250, cost: -87.148\n",
      "on iter 500, cost: -86.102\n",
      "on iter 750, cost: -87.583\n",
      "epoch 28\n",
      "on iter 0, cost: -87.676\n",
      "on iter 250, cost: -84.821\n",
      "on iter 500, cost: -84.438\n",
      "on iter 750, cost: -86.353\n",
      "epoch 29\n",
      "on iter 0, cost: -87.839\n",
      "on iter 250, cost: -87.603\n",
      "on iter 500, cost: -80.519\n",
      "on iter 750, cost: -87.077\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f983469cf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    test_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
